{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "git_main.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_Qjn1F4LfEH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "#drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sc-5GOQTT-Tl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/reco/')\n",
        "#from content.drive.reco.data.start import start\n",
        "import pandas as pd\n",
        "data = pd.read_csv('/path/to/ratings.csv', header=0)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3Cylwd2fJTH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#data.iloc[3]\n",
        "data = data[data[\"rating\"]>=4]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTDcyaUc9JvB",
        "colab_type": "text"
      },
      "source": [
        "Remove users with less than 5 interactions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kY0HzRuB7dwX",
        "colab_type": "code",
        "outputId": "3e8d89c6-b3d1-481a-f29f-b86fdd8eea67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "movie_dist = data[[\"movieId\"]].groupby(\"movieId\",as_index=False).size()\n",
        "user_dist = data[[\"userId\"]].groupby(\"userId\",as_index=False).size()\n",
        "data = data[data['userId'].isin(user_dist.index[user_dist >= 5])]\n",
        "\n",
        "# Update movie and user distribution\n",
        "\n",
        "movie_dist = data[[\"movieId\"]].groupby(\"movieId\",as_index=False).size()\n",
        "user_dist = data[[\"userId\"]].groupby(\"userId\",as_index=False).size()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20720, 136677)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYcs_1gtEanU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "user_list = user_dist.index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mt3jaialMTVe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#user_list\n",
        "\n",
        "user_dist=user_dist.sample(frac=1)\n",
        "user_list = user_dist.index\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UjM3rbeOJp9",
        "colab_type": "text"
      },
      "source": [
        "Straight Forward Splitting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Des481iwqxgC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "user_list_tr = user_list[:-20000]\n",
        "\n",
        "user_list_te=user_list[-20000:-10000]\n",
        "user_list_vd = user_list[-10000:]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGFFlQbdO7M3",
        "colab_type": "text"
      },
      "source": [
        "Done split for the moment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vyhUmLZO-or",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_tr = data[data['userId'].isin(user_list_tr)]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhCuXAX4SBWy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "unique_movie = pd.unique(data_tr['movieId'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MS9Q66NGUVev",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "movie2id = dict((sid,i) for (i, sid) in enumerate(unique_movie))\n",
        "user2id = dict((sid,i) for (i,sid) in enumerate(user_list))\n",
        "#movie2id has movies only in the traning dataset whereas user2id has all the users"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iC6FW9owdYc5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#save the lists\n",
        "import os\n",
        "data_root = '/Root folder to save the data/'\n",
        "save_dir = data_root+ 'lists'\n",
        "if not os.path.exists(save_dir):\n",
        "  os.mkdir(save_dir)\n",
        "with open(os.path.join(save_dir, 'unique_movie.txt'), 'w') as f:\n",
        "    for movie in unique_movie:\n",
        "        f.write('%s\\n' % movie)\n",
        "#for movie in unique_movie:\n",
        "#  print (movie)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2bXD89afz3I",
        "colab_type": "text"
      },
      "source": [
        "saved movie list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMZwJ3Lpf57-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def numerize(dt):\n",
        "    uid = list(map(lambda x: user2id[x], dt['userId']))\n",
        "    sid = list(map(lambda x: movie2id[x], dt['movieId']))\n",
        "    return pd.DataFrame(data={'uid': uid, 'sid': sid}, columns=['uid', 'sid'])\n",
        "#train_data = numerize(data_tr)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JsH-oUjGnv22",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = numerize(data_tr)\n",
        "train_data.to_csv(os.path.join(save_dir, 'train.csv'), index=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiIZrgP45QMS",
        "colab_type": "text"
      },
      "source": [
        "the train_data only contain the mapping of the users and movies where the actual user and movies are in the user2id, movie2id lists\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2WQuoZf6Hc4",
        "colab_type": "text"
      },
      "source": [
        "Only val split atm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dj4Lv3kDZxYH",
        "colab_type": "text"
      },
      "source": [
        "ATM only develop single model :\n",
        "denoising autoincoders with multinomial likelihood fn\n",
        "Why: Cause the time dude"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwISA1ZJ9tcn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load data and train\n",
        "\n",
        "#unique_movie\n",
        "from scipy import sparse\n",
        "\n",
        "num_item = len(unique_movie)\n",
        "num_item\n",
        "len(user_list_tr)\n",
        "n_users=data_tr['userId'].max() + 1\n",
        "#data_tr['userId'].head()\n",
        "rows, cols = train_data['uid'], train_data['sid']\n",
        "training_data = sparse.csr_matrix((np.ones_like(rows),(rows, cols)), dtype='float64',shape=(n_users, num_item))\n",
        "# data has entry 1 where the user has rated the movie: highly sparse matrix\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t52zwMcb7I1i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_train_test_proportion(data, test_prop=0.2):\n",
        "    data_grouped_by_user = data.groupby('userId')\n",
        "    tr_list, te_list = list(), list()\n",
        "\n",
        "    np.random.seed(98765)\n",
        "\n",
        "    for i, (_, group) in enumerate(data_grouped_by_user):\n",
        "        n_items_u = len(group)\n",
        "\n",
        "        if n_items_u >= 5:\n",
        "            idx = np.zeros(n_items_u, dtype='bool')\n",
        "            idx[np.random.choice(n_items_u, size=int(test_prop * n_items_u), replace=False).astype('int64')] = True\n",
        "\n",
        "            tr_list.append(group[np.logical_not(idx)])\n",
        "            te_list.append(group[idx])\n",
        "        else:\n",
        "            tr_list.append(group)\n",
        "\n",
        "        \n",
        "\n",
        "    data_tr = pd.concat(tr_list)\n",
        "    data_te = pd.concat(te_list)\n",
        "    \n",
        "    return data_tr, data_te"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuX3KdU_7gIY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_test = data.loc[data['userId'].isin(user_list_te)]\n",
        "data_test = data_test.loc[data['movieId'].isin(unique_movie)]\n",
        "data_test_tr,data_test_te = split_train_test_proportion(data_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHYVSfaUL6Y6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_test_tr = numerize(data_test_tr)\n",
        "data_test_te = numerize(data_test_te)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhUIz57X-2zd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_test_tr.to_csv(os.path.join(save_dir, 'data_test_tr.csv'), index=False)\n",
        "data_test_te.to_csv(os.path.join(save_dir, 'data_test_te.csv'), index=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58M7Pjmy8iSb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_vd = data.loc[data['userId'].isin(user_list_vd)]\n",
        "data_vd = data_vd.loc[data['movieId'].isin(unique_movie)]\n",
        "data_vd_tr,data_vd_te = split_train_test_proportion(data_vd)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLiCBsE-MCvU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_vd_tr = numerize(data_vd_tr)\n",
        "data_vd_te = numerize(data_vd_te)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ijvnk0fV_HbN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_vd_tr.to_csv(os.path.join(save_dir, 'data_vd_tr.csv'), index=False)\n",
        "data_vd_te.to_csv(os.path.join(save_dir, 'data_vd_te.csv'), index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcDpoDryqH0P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model in pytorch\n",
        "## l2 regularizers are included by default in optimizers??\n",
        "##will defining the model tf way later\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel # not using multi-gpu for the time being\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "class model_da(nn.Module):\n",
        "    def __init__(self,in_feature,latent_feature,weight_deca=0.0):\n",
        "      super(model_da, self).__init__()\n",
        "      self.weight_decay = weight_deca\n",
        "      self.linear1 = nn.Linear(in_feature, latent_feature, bias=True)\n",
        "      self.linear2 = nn.Linear(latent_feature, in_feature, bias=True)\n",
        "      self.tanh = nn.Tanh()\n",
        "    def forward(self,inp,drop_prob=0):\n",
        "      x1 = torch.nn.functional.normalize(inp,p=2,dim=1) #default 2nd degree normalizer\n",
        "      x2 = torch.nn.Dropout(p=drop_prob)(x1)\n",
        "      x_lat = self.tanh(self.linear1(x2))\n",
        "\n",
        "      x_out = self.linear2(x_lat)\n",
        "\n",
        "      return x_out\n",
        "    def loss(self,pred,gt):\n",
        "      pred = nn.functional.log_softmax(pred,dim=-1)\n",
        "      loss_mat = pred*gt\n",
        "      loss = -loss_mat.mean(1)\n",
        "      return loss.mean(0).view(1)\n",
        "    def l2_reg(self):\n",
        "        l2_reg = Variable(torch.FloatTensor(1), requires_grad=True)\n",
        "        \n",
        "        for k, m in self.state_dict().items():\n",
        "            if k.endswith('.weight'):\n",
        "                l2_reg = l2_reg + torch.norm(m, p=2) ** 2\n",
        "            l2_reg = self.weight_decay * l2_reg\n",
        "        #if self.cuda2:\n",
        "        #    l2_reg = l2_reg.cuda()\n",
        "        return l2_reg[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQKQ6uhLeOGr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_data = training_data\n",
        "N = training_data.shape[0]\n",
        "idxlist = range(N)\n",
        "batch_size = 500\n",
        "batches_per_epoch = int(np.ceil(float(N) / batch_size))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPP30RboMpdU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_tr_te_data(tp_tr, tp_te):\n",
        "    #tp_tr = pd.read_csv(csv_file_tr)\n",
        "    #tp_te = pd.read_csv(csv_file_te)\n",
        "\n",
        "\n",
        "    start_idx = min(tp_tr['uid'].min(), tp_te['uid'].min())\n",
        "    end_idx = max(tp_tr['uid'].max(), tp_te['uid'].max())\n",
        "\n",
        "    rows_tr, cols_tr = tp_tr['uid'] - start_idx, tp_tr['sid']\n",
        "    rows_te, cols_te = tp_te['uid'] - start_idx, tp_te['sid']\n",
        "\n",
        "    data_tr = sparse.csr_matrix((np.ones_like(rows_tr),\n",
        "                             (rows_tr, cols_tr)), dtype='float64', shape=(end_idx - start_idx + 1, num_item))\n",
        "    data_te = sparse.csr_matrix((np.ones_like(rows_te),\n",
        "                             (rows_te, cols_te)), dtype='float64', shape=(end_idx - start_idx + 1, num_item))\n",
        "    return data_tr, data_te"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZFtskYBM_OQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vad_data_tr, vad_data_te = load_tr_te_data(data_vd_tr,data_vd_te)\n",
        "                              "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRh7Ls0rPqfT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import bottleneck as bn\n",
        "def NDCG_binary_at_k_batch(X_pred, heldout_batch, k=100):\n",
        "    '''\n",
        "    normalized discounted cumulative gain@k for binary relevance\n",
        "    ASSUMPTIONS: all the 0's in heldout_data indicate 0 relevance\n",
        "    '''\n",
        "    batch_users = X_pred.shape[0]\n",
        "    idx_topk_part = bn.argpartition(-X_pred, k, axis=1)\n",
        "    topk_part = X_pred[np.arange(batch_users)[:, np.newaxis],\n",
        "                       idx_topk_part[:, :k]]\n",
        "    idx_part = np.argsort(-topk_part, axis=1)\n",
        "    # X_pred[np.arange(batch_users)[:, np.newaxis], idx_topk] is the sorted\n",
        "    # topk predicted score\n",
        "    idx_topk = idx_topk_part[np.arange(batch_users)[:, np.newaxis], idx_part]\n",
        "    # build the discount template\n",
        "    tp = 1. / np.log2(np.arange(2, k + 2))\n",
        "\n",
        "    DCG = (heldout_batch[np.arange(batch_users)[:, np.newaxis],\n",
        "                         idx_topk].toarray() * tp).sum(axis=1)\n",
        "    IDCG = np.array([(tp[:min(n, k)]).sum()\n",
        "                     for n in heldout_batch.getnnz(axis=1)])\n",
        "    return DCG / IDCG"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XnnkwNP32yq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# weight initialization\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Linear') != -1:\n",
        "        torch.nn.init.xavier_uniform_(m.weight.data)\n",
        "        torch.nn.init.normal_(m.bias.data,std=0.001)\n",
        "        m.bias.data.clamp_(-2*0.001, 2*0.001) # temporary fix: this will set the values to 0 but \n",
        "        # in tf the values are redrawn : write a function for this if necessary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nv5BYkrP05y6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#call model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model_da(num_item,200,weight_deca=0.01 /500)\n",
        "model.apply(weights_init)\n",
        "\n",
        "model.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=0.0)\n",
        "inputs = torch.FloatTensor(batch_size,num_item )\n",
        "out_gr = torch.FloatTensor(batch_size,num_item )\n",
        "\n",
        "inputs.to(device)\n",
        "out_gr.to(device)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atblfXDeV9yD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#define default pytorch dataloader\n",
        "from torch.utils import data\n",
        "from scipy import sparse\n",
        "# assumes 'training_data' is np array\n",
        "# need to chage the structure of array from [a,b] to [a,[b,1]]\n",
        "#training_data= sparse.csr_matrix.toarray(training_data)\n",
        "dataloader= torch.utils.data.DataLoader(training_data,batch_size = 500,drop_last=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ry3xvTP7ZLxu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#for validation data\n",
        "batch_size_vd = batch_size\n",
        "no_batches = 10000/batch_size_vd\n",
        "batch_list_vd = np.arange((no_batches)).astype('int32')\n",
        "rng = np.random.default_rng()\n",
        "rng.shuffle(batch_list_vd)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "src077usVP00",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loop for training\n",
        "training_data\n",
        "138494/500\n",
        "batches_per_epoch\n",
        "batch_list = np.arange((batches_per_epoch-2))\n",
        "rng = np.random.default_rng()\n",
        "#batch_list = np.array(batch_list)\n",
        "#list(range(batch_list))\n",
        "#print (batch_list)\n",
        "rng.shuffle(batch_list)\n",
        "#print (batch_list)\n",
        "ndcg_vad =[]\n",
        "from torch.autograd import Variable\n",
        "ndcgs_vad=[]\n",
        "for epoch in range(200):\n",
        "  #data_iter = iter(dataloader)\n",
        "  \n",
        "  rng.shuffle(batch_list)\n",
        "  #print (batch_list)\n",
        "  i = 0\n",
        "  while i < len(batch_list):\n",
        "    #data = data_iter.next()\n",
        "    data = training_data[batch_list[i]*batch_size:(batch_list[i]+1)*batch_size].toarray()\n",
        "    \n",
        "    i+=1\n",
        "    \n",
        "    model.zero_grad()\n",
        "    inputs=data\n",
        "    inputv = Variable(torch.Tensor(inputs)).to(device)\n",
        "    #print (inputv.shape)\n",
        "    pred = model(inputv)\n",
        "    los = model.loss(pred,inputv) \n",
        "    los_reg = model.l2_reg().to(device)\n",
        "    loss_total = los+los_reg\n",
        "    loss_total.backward()\n",
        "    optimizer.step()\n",
        "  ndcg_dist = []\n",
        "  j=0\n",
        "  while j< len(batch_list_vd):\n",
        "    \n",
        "    \n",
        "    data_vd_tr = vad_data_tr[batch_list_vd[j]*batch_size:(batch_list_vd[j]+1)*batch_size].toarray()\n",
        "    data_vd_te = vad_data_te[batch_list_vd[j]*batch_size:(batch_list_vd[j]+1)*batch_size]#.toarray()\n",
        "    with torch.no_grad():\n",
        "      pred_val = model(Variable(Variable(torch.Tensor(data_vd_tr)).to(device)))\n",
        "      pred_val = pred_val.cpu().detach().numpy()\n",
        "      pred_val[data_vd_tr.nonzero()] = -np.inf\n",
        "      ndcg_dist.append(NDCG_binary_at_k_batch(pred_val, data_vd_te))\n",
        "    j+=1\n",
        "  ndcg_dist = np.concatenate(ndcg_dist)\n",
        "  ndcg_ = ndcg_dist.mean()\n",
        "  #print (ndcg_)\n",
        "  ndcg_vad.append(ndcg_)\n",
        "\n",
        "  \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ce25iYM9hm7i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "x = np.arange(1,201,1)\n",
        "\n",
        "plt.figure(figsize=(12, 3))\n",
        "plt.plot(x,ndcgs_vad,'r-',label='final')\n",
        "plt.ylabel(\"Validation NDCG@100\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.legend()\n",
        "plt.savefig(save_dir+'/combine')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}